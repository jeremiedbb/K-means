{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from numba import jit, njit, prange, float32\n",
    "import numpy as np\n",
    "from joblib import Memory\n",
    "from sklearn.cluster.k_means_ import _k_init\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Memory(location='/tmp/joblib')\n",
    "make_blobs = m.cache(make_blobs)\n",
    "_k_init = m.cache(_k_init)\n",
    "n_clusters = 1000\n",
    "rng = check_random_state(42)\n",
    "kmeanspp_size = int(1e4)\n",
    "\n",
    "data, true_labels = make_blobs(n_samples=int(1e5), centers=100,\n",
    "                               n_features=100, cluster_std=30,\n",
    "                               random_state=rng)\n",
    "data = data.astype(np.float32)\n",
    "data_squared_norms = np.sum(data[:kmeanspp_size] * data[:kmeanspp_size],\n",
    "                            axis=1)\n",
    "centroids = _k_init(data[:kmeanspp_size], n_clusters,\n",
    "                    data_squared_norms, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('void(f4[:, ::1], f4[:, ::1], f4[:, ::1], u4[::1])',\n",
    "      locals={'best_dist': float32, 'dist': float32},\n",
    "      fastmath=True,\n",
    "#       parallel=True,\n",
    ")\n",
    "def kmeans_kernel(data, centroids, centroids_sum, centroids_pop):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_centroids = centroids.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        best_dist = 1e7\n",
    "        best_j = 0\n",
    "        for j in range(n_centroids):\n",
    "            dist = 0.\n",
    "            for k in range(n_features):\n",
    "                dist += (data[i, k] - centroids[j, k]) ** 2\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_j = j\n",
    "        for k in range(data.shape[1]):\n",
    "            centroids_sum[best_j, k] += data[i, k]\n",
    "        centroids_pop[best_j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 ms ± 1.78 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.uint32)\n",
    "kmeans_kernel(data_chunk, centroids, centroids_sum, centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most arithmetic intensive part is the nested for loop that computes the distance between one sample and one centroid. Each iteration of that loop has 3 floating point operations:\n",
    "\n",
    "- one difference\n",
    "- one multiplication (with self to compute a square)\n",
    "- one addition (accumulation in the dist variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "n_flop = 3 \n",
    "n_samples, n_features = data_chunk.shape\n",
    "n_centroids = centroids.shape[0]\n",
    "duration = 0.012  # measured by timeit\n",
    "\n",
    "gflop = (n_samples * n_centroids * n_features * 3) / 1e9\n",
    "print(f\"{gflop / duration:0.3} GFLOP/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 23 GFLOP/s on a single thread is very good. On this CPU (skylake), I think this is rather close to peak performance.\n",
    "\n",
    "However the same experiment with n_features=2 to n_features=30 gives much lower results from 2 to 5 GFLOPS. Note that we did not count the book-keeping of `best_dist` though and it starts to be relatively important for low values of `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kmeans_kernel.inspect_types(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans_kernel.inspect_asm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, the generated assembly should include lines with `vfmadd231ps` on `ymm` 256-bit avx registries meaning that one such instruction can pack 8 x (one multiplication and one addition) on float32 values (each `ymm` registry can pack 8 float32 values).\n",
    "\n",
    "The highly arithmetic calculation that can benefit from `vfmadd231ps` is in the python source:\n",
    "\n",
    "```python\n",
    "    dist += (data[i, k] - centroids[j, k]) ** 2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(next(iter(kmeans_kernel.inspect_asm().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit('u4(f4[:, ::1], f4[:, ::1], u4, u4)',\n",
    "     fastmath=True, parallel=True)\n",
    "def kmeans(data, centroids, chunk_size=1000, n_iter=10):\n",
    "    n_chunks, remainder = divmod(data.shape[0], chunk_size)\n",
    "    if remainder:\n",
    "        n_chunks += 1\n",
    "\n",
    "    centroids_sum = np.zeros_like(centroids)\n",
    "    centroids_pop = np.zeros(centroids.shape[0], dtype=np.uint32)\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        for i in prange(n_chunks):\n",
    "            data_chunk = data[i * chunk_size:(i + 1) * chunk_size]\n",
    "            kmeans_kernel(data_chunk, centroids, centroids_sum, centroids_pop)\n",
    "        # TODO: parallel accumulate in nopython mode?\n",
    "        centroids[:] = centroids_sum / centroids_pop[:, np.newaxis]\n",
    "    return n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s ± 59.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "kmeans(data, centroids, chunk_size=1000, n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "import os\n",
    "os.environ[\"CC\"] = 'gcc-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -c=-Ofast -c=-march=native -f\n",
    "#cython: initializedcheck=False, nonecheck=False, boundscheck=False, wraparound=False, cdivision=True, overflowcheck=False, overflowcheck.fold=False\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "from scipy.linalg.cython_blas cimport sgemm\n",
    "\n",
    "ctypedef fused floating_t:\n",
    "    np.float32_t\n",
    "    np.float64_t\n",
    "\n",
    "# numpy array\n",
    "cpdef void kmeans_chunk_np(np.ndarray[floating_t, ndim=2, mode='c'] X_chunk,\n",
    "                           np.ndarray[floating_t, ndim=2, mode='c'] C,\n",
    "                           np.ndarray[floating_t, ndim=2, mode='c'] sums,\n",
    "                           np.ndarray[np.int32_t, ndim=1, mode='c'] pops):\n",
    "    cdef:\n",
    "        Py_ssize_t n_samples_chunk = X_chunk.shape[0],\n",
    "        Py_ssize_t n_clusters = C.shape[0],\n",
    "        Py_ssize_t n_features = C.shape[1]\n",
    "        \n",
    "        floating_t x, sq_dist, min_sq_dist = 0.0\n",
    "        Py_ssize_t best_cluster = -1\n",
    "\n",
    "        Py_ssize_t si, ci, fi = 0\n",
    "\n",
    "    for si in xrange(n_samples_chunk):\n",
    "        min_sq_dist = 10000000\n",
    "        best_cluster = -1\n",
    "        for ci in xrange(n_clusters):\n",
    "            sq_dist = 0.0\n",
    "            for fi in xrange(n_features):\n",
    "                x = X_chunk[si, fi] - C[ci, fi]\n",
    "                sq_dist += x * x\n",
    "            if sq_dist < min_sq_dist:\n",
    "                min_sq_dist = sq_dist\n",
    "                best_cluster = ci\n",
    "\n",
    "        pops[best_cluster] += 1 \n",
    "        for fi in xrange(n_features):  \n",
    "            sums[best_cluster, fi] += X_chunk[si, fi]\n",
    "\n",
    "# pointer   \n",
    "cdef void kmeans_chunk_ptr(floating_t *X_chunk,\n",
    "                           floating_t *C,\n",
    "                           floating_t *sums,\n",
    "                           np.int32_t *pops,\n",
    "                           Py_ssize_t n_samples_chunk,\n",
    "                           Py_ssize_t n_clusters,\n",
    "                           Py_ssize_t n_features) nogil:\n",
    "    cdef:\n",
    "        floating_t x, sq_dist, min_sq_dist = 0.0\n",
    "        np.int32_t best_cluster = -1\n",
    "\n",
    "        Py_ssize_t si, ci, fi = 0\n",
    "\n",
    "    for si in xrange(n_samples_chunk):\n",
    "        min_sq_dist = 10000000\n",
    "        best_cluster = -1\n",
    "        for ci in xrange(n_clusters):\n",
    "            sq_dist = 0.0\n",
    "            for fi in xrange(n_features):\n",
    "                x = X_chunk[si * n_features + fi] - C[ci * n_features + fi]\n",
    "                sq_dist += x * x\n",
    "            if sq_dist < min_sq_dist:\n",
    "                min_sq_dist = sq_dist\n",
    "                best_cluster = ci\n",
    "\n",
    "        pops[best_cluster] += 1             \n",
    "        for fi in xrange(n_features):    \n",
    "            sums[best_cluster * n_features + fi] += X_chunk[si * n_features + fi]\n",
    "        \n",
    "\n",
    "cpdef kmeans_chunk_ptrw(floating_t[:, ::1] X_chunk,\n",
    "                        floating_t[:, ::1] C,\n",
    "                        floating_t[:, ::1] sums,\n",
    "                        np.int32_t[::1] pops):\n",
    "    cdef:\n",
    "        Py_ssize_t n_samples_chunk = X_chunk.shape[0]\n",
    "        Py_ssize_t n_features = X_chunk.shape[1]\n",
    "        Py_ssize_t n_clusters = C.shape[0]\n",
    "        \n",
    "    kmeans_chunk_ptr(&X_chunk[0,0], &C[0,0], &sums[0,0], &pops[0], n_samples_chunk, n_clusters, n_features)\n",
    "    \n",
    "    \n",
    "# memoryview          \n",
    "cpdef void kmeans_chunk_mv(floating_t[:, ::1] X_chunk,\n",
    "                           floating_t[:, ::1] C,\n",
    "                           floating_t[:, ::1] sums,\n",
    "                           np.int32_t[::1] pops) nogil:\n",
    "    cdef:\n",
    "        Py_ssize_t n_samples_chunk = X_chunk.shape[0]\n",
    "        Py_ssize_t n_clusters = C.shape[0]\n",
    "        Py_ssize_t n_features = X_chunk.shape[1]\n",
    "\n",
    "        floating_t x, sq_dist, min_sq_dist = 0.0\n",
    "        Py_ssize_t best_cluster = -1\n",
    "\n",
    "        Py_ssize_t si, ci, fi = 0\n",
    "\n",
    "    for si in xrange(n_samples_chunk):\n",
    "        min_sq_dist = 10000000\n",
    "        best_cluster = -1\n",
    "        for ci in xrange(n_clusters):\n",
    "            sq_dist = 0.0\n",
    "            for fi in xrange(n_features):\n",
    "                x = X_chunk[si, fi] - C[ci, fi]\n",
    "                sq_dist += x * x\n",
    "            if sq_dist < min_sq_dist:\n",
    "                min_sq_dist = sq_dist\n",
    "                best_cluster = ci\n",
    "                \n",
    "        pops[best_cluster] += 1          \n",
    "        for fi in xrange(n_features):      \n",
    "            sums[best_cluster, fi] += X_chunk[si, fi]\n",
    "            \n",
    "\n",
    "# memoryview 2    \n",
    "cpdef void kmeans_chunk_gemm(np.float32_t[:, ::1] X_chunk,\n",
    "                             np.float32_t[:, ::1] C,\n",
    "                             np.float32_t[::1] C_snorms,\n",
    "                             np.float32_t[:, ::1] sums,\n",
    "                             np.int32_t[::1] pops):\n",
    "    cdef:\n",
    "        int n_samples_chunk = X_chunk.shape[0]\n",
    "        int n_clusters = C.shape[0]\n",
    "        int n_features = X_chunk.shape[1]\n",
    "\n",
    "        np.float32_t x, sq_dist, min_sq_dist = 0.0\n",
    "        int best_cluster = -1\n",
    "\n",
    "        int si, ci, fi, fc= 0\n",
    "        \n",
    "        np.float32_t[:, ::1] dots = np.zeros((n_samples_chunk, n_clusters), dtype=np.float32)\n",
    "        \n",
    "        np.float32_t alpha = 1.0\n",
    "        np.float32_t beta = 0.0\n",
    "        char *transa = 'n'\n",
    "        char *transb = 't'\n",
    "        np.float32_t *a0=&X_chunk[0,0]\n",
    "        np.float32_t *b0=&C[0,0]\n",
    "        np.float32_t *c0=&dots[0,0]\n",
    "        \n",
    "    sgemm(transb, transa, &n_clusters, &n_samples_chunk, &n_features, &alpha, b0, &n_features, a0, &n_features, &beta, c0, &n_clusters)\n",
    "        \n",
    "    for si in xrange(n_samples_chunk):\n",
    "        min_sq_dist = 10000000.0\n",
    "        best_cluster = -1\n",
    "        for ci in xrange(n_clusters):\n",
    "            sq_dist = C_snorms[ci] + dots[si, ci]\n",
    "            if sq_dist < min_sq_dist:\n",
    "                min_sq_dist = sq_dist\n",
    "                best_cluster = ci\n",
    "                \n",
    "        pops[best_cluster] += 1          \n",
    "        for fi in xrange(n_features):      \n",
    "            sums[best_cluster, fi] += X_chunk[si, fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "centroids = centroids.astype(np.float32)\n",
    "centroids_snorms = (centroids**2).sum(axis=1) * -0.5\n",
    "data_chunk = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 ms ± 599 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_np(data_chunk,\n",
    "                centroids,\n",
    "                centroids_sum,\n",
    "                centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 ms ± 1.67 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_ptrw(data_chunk,\n",
    "                  centroids,\n",
    "                  centroids_sum,\n",
    "                  centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_mv(data_chunk,\n",
    "                centroids,\n",
    "                centroids_sum,\n",
    "                centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memoryview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63 ms ± 148 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_gemm(data_chunk,\n",
    "                 centroids,\n",
    "                 centroids_snorms,\n",
    "                 centroids_sum,\n",
    "                 centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5ms with gemm !\n",
    "\n",
    "n_samples * n_clusters * n_features * 2 = 2.10⁸ flop\n",
    "\n",
    "~ 55 Gflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float64)\n",
    "centroids = centroids.astype(np.float64)\n",
    "data_chunk = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_np(data_chunk,\n",
    "                centroids,\n",
    "                centroids_sum,\n",
    "                centroids_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_ptrw(data_chunk,\n",
    "                  centroids,\n",
    "                  centroids_sum,\n",
    "                  centroids_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.int32)\n",
    "kmeans_chunk_mv(data_chunk,\n",
    "                centroids,\n",
    "                centroids_sum,\n",
    "                centroids_pop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
