{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from numba import jit, njit, prange, float32\n",
    "import numpy as np\n",
    "from joblib import Memory\n",
    "from sklearn.cluster.k_means_ import _k_init\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Memory(location='/tmp/joblib')\n",
    "make_blobs = m.cache(make_blobs)\n",
    "_k_init = m.cache(_k_init)\n",
    "n_clusters = 1000\n",
    "rng = check_random_state(42)\n",
    "kmeanspp_size = int(1e4)\n",
    "\n",
    "data, true_labels = make_blobs(n_samples=int(1e5), centers=100,\n",
    "                               n_features=100, cluster_std=30,\n",
    "                               random_state=rng)\n",
    "data = data.astype(np.float32)\n",
    "data_squared_norms = np.sum(data[:kmeanspp_size] * data[:kmeanspp_size],\n",
    "                            axis=1)\n",
    "centroids = _k_init(data[:kmeanspp_size], n_clusters,\n",
    "                    data_squared_norms, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('void(f4[:, ::1], f4[:, ::1], f4[:, ::1], u4[::1])',\n",
    "      locals={'best_dist': float32, 'dist': float32},\n",
    "      fastmath=True,\n",
    "#       parallel=True,\n",
    ")\n",
    "def kmeans_kernel(data, centroids, centroids_sum, centroids_pop):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_centroids = centroids.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        best_dist = 1e7\n",
    "        best_j = 0\n",
    "        for j in range(n_centroids):\n",
    "            dist = 0.\n",
    "            for k in range(n_features):\n",
    "                dist += (data[i, k] - centroids[j, k]) ** 2\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_j = j\n",
    "        for k in range(data.shape[1]):\n",
    "            centroids_sum[best_j, k] += data[i, k]\n",
    "        centroids_pop[best_j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk = data[:1000]\n",
    "centroids_sum = np.zeros_like(centroids)\n",
    "centroids_pop = np.zeros(centroids.shape[0], dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3 ms ± 1.94 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "kmeans_kernel(data_chunk, centroids, centroids_sum, centroids_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most arithmetic intensive part is the nested for loop that computes the distance between one sample and one centroid. Each iteration of that loop has 3 floating point operations:\n",
    "\n",
    "- one difference\n",
    "- one multiplication (with self to compute a square)\n",
    "- one addition (accumulation in the dist variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "n_flop = 3 \n",
    "n_samples, n_features = data_chunk.shape\n",
    "n_centroids = centroids.shape[0]\n",
    "duration = 0.013  # measured by timeit\n",
    "\n",
    "gflop = (n_samples * n_centroids * n_features * 3) / 1e9\n",
    "print(f\"{gflop / duration:0.3} GFLOP/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 23 GFLOP/s on a single thread is very good. On this CPU (skylake), I think this is rather close to peak performance.\n",
    "\n",
    "However the same experiment with n_features=2 to n_features=30 gives much lower results from 2 to 5 GFLOPS. Note that we did not count the book-keeping of `best_dist` though and it starts to be relatively important for low values of `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kmeans_kernel.inspect_types(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans_kernel.inspect_asm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, the generated assembly should include lines with `vfmadd231ps` on `ymm` 256-bit avx registries meaning that one such instruction can pack 8 x (one multiplication and one addition) on float32 values (each `ymm` registry can pack 8 float32 values).\n",
    "\n",
    "The highly arithmetic calculation that can benefit from `vfmadd231ps` is in the python source:\n",
    "\n",
    "```python\n",
    "    dist += (data[i, k] - centroids[j, k]) ** 2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(next(iter(kmeans_kernel.inspect_asm().values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit('u4(f4[:, ::1], f4[:, ::1], u4, u4)',\n",
    "     fastmath=True, parallel=True)\n",
    "def kmeans(data, centroids, chunk_size=1000, n_iter=10):\n",
    "    n_chunks, remainder = divmod(data.shape[0], chunk_size)\n",
    "    if remainder:\n",
    "        n_chunks += 1\n",
    "\n",
    "    centroids_sum = np.zeros_like(centroids)\n",
    "    centroids_pop = np.zeros(centroids.shape[0], dtype=np.uint32)\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        for i in prange(n_chunks):\n",
    "            data_chunk = data[i * chunk_size:(i + 1) * chunk_size]\n",
    "            kmeans_kernel(data_chunk, centroids, centroids_sum, centroids_pop)\n",
    "        # TODO: parallel accumulate in nopython mode?\n",
    "        centroids[:] = centroids_sum / centroids_pop[:, np.newaxis]\n",
    "    return n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 s ± 16.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "kmeans(data, centroids, chunk_size=1000, n_iter=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
